<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Yash Bhalgat</title>

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/yash_logo.png">

  <style>
    u.dotted {
      border-bottom: 1px dotted #000;
      text-decoration: none;
    }
  </style>
</head>

<body>

  <nav class="navbar">
    <button class="nav-toggle">
        <span></span>
        <span></span>
        <span></span>
    </button>
    <ul>
      <li><a href="#news">News</a></li>
      <li><a href="#research">Research</a></li>
      <li><a href="#invited-talks">Invited Talks</a></li>
      <li><a href="#hall-of-fame">Hall of Fame</a></li>
      <li><a href="#music">Music</a></li>
      <li><a href="#office-hours">Office Hours</a></li>
      <li><a href=blog.html>Blog</a></li>
    </ul>
  </nav>

  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle" align="justify">
                  <p style="text-align:center">
                    <name>यश भळगट | Yash Bhalgat</name>
                  </p>
                  <p>
                    Final year PhD candidate in the <a href="https://www.robots.ox.ac.uk/~vgg/" target="_blank"
                      rel="noopener noreferrer">Visual Geometry Group (VGG)</a>, Oxford. Co-advised by <a
                      href="https://www.robots.ox.ac.uk/~az/" target="_blank" rel="noopener noreferrer">Andrew
                      Zisserman</a>, <a href="https://www.robots.ox.ac.uk/~vedaldi/" target="_blank"
                      rel="noopener noreferrer">Andrea Vedaldi</a>, <a href="https://www.robots.ox.ac.uk/~joao/"
                      target="_blank" rel="noopener noreferrer">Jo&atilde;o Henriques</a> and <a
                      href="https://scholar.google.de/citations?user=n9nXAPcAAAAJ&hl=en" target="_blank"
                      rel="noopener noreferrer">Iro Laina</a>. Funded by the EPSRC+AWS fellowship with <a
                      href="https://aims.robots.ox.ac.uk/" target="_blank" rel="noopener noreferrer">AIMS CDT</a>.
                  </p>
                  <p style="margin-bottom: 0;">Research Interests:</p>
                  <ul style="margin-top: 0;">
                    <li><strong>3D Computer Vision</strong>: <a
                        href="https://arxiv.org/abs/2306.04633">Contrastive-Lift</a>, <a
                        href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bhalgat_A_Light_Touch_Approach_to_Teaching_Transformers_Multi-View_Geometry_CVPR_2023_paper.pdf">Light-Touch</a>,
                      <a
                        href="https://openaccess.thecvf.com/content/WACV2022/html/Yang_Dynamic_Iterative_Refinement_for_Efficient_3D_Hand_Pose_Estimation_WACV_2022_paper.html">DIR-Net</a>
                    </li>
                    <li><strong>Vision + Language</strong>: <a href="https://arxiv.org/abs/2403.10997">N2F2</a>, <a
                        href="https://arxiv.org/abs/2405.10255">3D-LLMs</a></li>
                    <li><strong>Efficient Machine Learning</strong>: <a
                        href="https://openaccess.thecvf.com/content_CVPRW_2020/html/w40/Bhalgat_LSQ_Improving_Low-Bit_Quantization_Through_Learnable_Offsets_and_Better_Initialization_CVPRW_2020_paper.html">LSQ+</a>,
                      <a href="https://arxiv.org/abs/1911.12491">QKD</a>, <a
                        href="https://arxiv.org/abs/2008.02454">StructConv</a>, <a
                        href="https://arxiv.org/pdf/2003.00075.pdf">LTP</a></li>
                  </ul>
                  <p>
                    Parallelly, I also work as an AI consultant for an <i>on-device AI</i> startup and a <i>LLM content
                      moderation</i> company.
                    Before, I was a Senior Researcher at <a
                      href="https://www.qualcomm.com/invention/artificial-intelligence/ai-research" target="_blank"
                      rel="noopener noreferrer">Qualcomm AI Research</a>. I have also been fortunate to spend time at <a
                      href="https://voxel51.com/" target="_blank" rel="noopener noreferrer">Voxel51</a>, IBM Research
                    (Bangalore and Almaden Lab), IFPEN (Paris), TCS Research.
                    <!-- and a CV/ML Engineer at <a href="https://voxel51.com/" target="_blank" rel="noopener noreferrer">Voxel51</a>. -->
                  </p>
                  <!-- <p>
        I have interned at IBM Research (Bangalore and Almaden, California), IFPEN (Paris), TCS Research (Pune).
        </p> -->
                  <p style="margin-bottom: 0;"><strong>Education:</strong></p>
                  <ul style="margin-top: 0;">
                    <li>Masters in Computer Science from the <a
                        href="https://cse.engin.umich.edu/academics/undergraduate/computer-science-eng/" target="_blank"
                        rel="noopener noreferrer">University of Michigan</a></li>
                    <li>Bachelors in Electrical Engineering (CS minor) from <a href="http://iitb.ac.in/" target="_blank"
                        rel="noopener noreferrer">IIT Bombay</a></li>
                  </ul>
                  <p>
                    Feel free to <u class="dotted"><b><i><a href="https://cal.com/yash-bhalgat" target="_blank" rel="noopener noreferrer"
                      style="color:rgba(255, 0, 174, 0.804)">setup a call</a></b></i></u> if you want to discuss ideas around startups, AI (CV / ML / LLMs), or want to collaborate.
                  </p>
                  <!-- <p>
                    <span style="color:red;"><b>Open to research internships for 2025 - feel free to connect if my work interests you!</b></span>
                  </p> -->
                  <p style="text-align:center" class="large-text">
                    <a href="mailto:yashbhalgat95@gmail.com" class="large-link">Email</a> &nbsp/&nbsp
                    <a href="data/CV_YashBhalgat_Jun2025.pdf" class="large-link">CV</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?hl=en&user=q0VSEHYAAAAJ"
                      class="large-link">Scholar</a> &nbsp/&nbsp
                    <a href="https://github.com/yashbhalgat" class="large-link">Github</a> &nbsp/&nbsp
                    <a href="https://www.linkedin.com/in/yashbhalgat/" class="large-link">LinkedIn</a> &nbsp/&nbsp
                    <a href="https://twitter.com/ysbhalgat" class="large-link">X</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/YashBhalgat_Ghibli.png"><img style="width:80%;max-width:100%" alt="profile photo"
                      src="images/YashBhalgat_Ghibli.png" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <!-- <td width="10%" valign="middle">
                <a href="https://ethz.ch/en.html"><img src="media/logo_deepmind.png" width="60"></a>
              </td> -->
              <td width="16%" valign="middle">
                <a href="https://www.ox.ac.uk/"><img src="data/oxford_logo.png" width="75"></a>
              </td>
              <td width="16%" valign="middle">
                <a href="https://www.qualcomm.com/research/artificial-intelligence/ai-research"><img src="data/qualcomm_logo.png" width="180"></a>
              </td>
              <td width="16%" valign="middle">
                <a href="https://voxel51.com/"><img src="data/voxel51_logo.png" width="90"></a>
              </td>
              <td width="16%" valign="middle">
                <a href="https://research.ibm.com/labs/almaden"><img src="data/ibm_logo.webp" width="100"></a>
              </td>
              <td width="16%" valign="middle">
                <a href="https://cse.engin.umich.edu/"><img src="data/umich_logo.png" width="72"></a>
              </td>
              <td width="16%" valign="middle">
                <a href="https://www.iitb.ac.in/"><img src="data/iitb_logo.png" width="80"></a>
              </td>
            </tr>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"
            id="news">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>News</heading>
                  <p>
                  <ul id="news">
                    <li><span style="color:blue;">[09/2025]</span> <a href="https://arxiv.org/abs/2506.08220">Jamais Vu</a> - Our paper on Semantic Correspondence Learning accepted to NeurIPS 2025!</li>
                    <li><span style="color:blue;">[06/2025]</span> Organized a successful <a href="https://3d-llm-vla.github.io/">1st Workshop on 3D-LLM/VLA</a> at CVPR 2025.</li>
                    <li><span style="color:blue;">[01/2025]</span> <a href="https://arxiv.org/abs/2408.11085">GSLoc</a> accepted to ICLR 2025!</li>
                    <li><span style="color:blue;">[11/2024]</span> Proud mentor moment &#128526;: <a href="https://arxiv.org/abs/2409.14677">Reflecting Reality</a>
                      accepted to 3DV 2025!</li>
                    <li><span style="color:blue;">[09/2024]</span> Our <a href="https://arxiv.org/abs/2408.09860">3D-Aware Egocentric Tracking</a>
                      paper accepted to ACCV 2024! &#127881;</li>
                    <li><span style="color:blue;">[07/2024]</span> <a href="https://arxiv.org/abs/2403.10997">N2F2</a>
                      accepted to ECCV 2024! &#127881;</li>
                    <li><span style="color:blue;">[02/2024]</span> <a href="https://arxiv.org/abs/2303.10087">NeFeS</a>
                      accepted to CVPR 2024! &#127881;</li>
                    <li><span style="color:blue;">[01/2024]</span> <a href="https://arxiv.org/abs/2403.06877">SiLVR</a>
                      accepted to ICRA 2024! &#127881;</li>
                    <li><span style="color:blue;">[01/2024]</span> We are organizing the 2nd Workshop on <a
                        href="https://abdullahamdi.com/3dmv2024/">Learning 3D with Multi-View Supervision</a> at
                      CVPR2024.</li>
                    <li><span style="color:blue;">[09/2023]</span> <a
                        href="https://arxiv.org/abs/2306.04633">Contrastive-Lift</a> paper accepted to NeurIPS 2023 as a
                      <b><span style="color:crimson;">Spotlight</span></b> presentation! &#128640; Code available: <a
                        href="https://github.com/yashbhalgat/Contrastive-Lift">github</a>.</li>
                    <li><span style="color:blue;">[03/2023]</span> <a
                        href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bhalgat_A_Light_Touch_Approach_to_Teaching_Transformers_Multi-View_Geometry_CVPR_2023_paper.pdf">Epipolar-guided
                        Transformers</a> paper accepted to CVPR 2023! &#127881;</li>
                    <li><span style="color:blue;">[01/2023]</span> <a
                        href="https://github.com/yashbhalgat/HashNeRF-pytorch">HashNeRF-Pytorch</a> hits 900+ stars on
                      Github! ⭐</li>
                    <li><span style="color:blue;">[06/2022]</span> Serving as a Website Chair for BMVC 2022.</li>
                    <li><span style="color:blue;">[10/2021]</span> <a
                        href="https://openaccess.thecvf.com/content/WACV2022/html/Yang_Dynamic_Iterative_Refinement_for_Efficient_3D_Hand_Pose_Estimation_WACV_2022_paper.html">3D
                        Hand Pose Estimation</a> work with my intern <a
                        href="https://www.linkedin.com/in/john-yang-b3a9b886/">John Yang</a> has been accepted to WACV
                      2022.</li>
                    <li><span style="color:red;font-weight: bold;">[10/2021]</span> Started my DPhil (PhD) at University
                      of Oxford in <a href="https://aims.robots.ox.ac.uk/">AIMS CDT</a>, funded by EPSRC+AWS fellowship.
                      &#128522;</li>
                    <li><span style="color:blue;">[11/2020]</span> I got promoted to Senior Machine Learning Researcher
                      at Qualcomm AI Research. &#128522;</li>
                    <li><span style="color:blue;">[09/2020]</span> <a
                        href="https://papers.nips.cc/paper/2020/hash/3be0214185d6177a9aa6adea5a720b09-Abstract.html">Structured
                        Convolutions</a> paper accepted to NeurIPS 2020! </li>
                    <li><span style="color:blue;">[03/2020]</span> LSQ+ <a
                        href="https://openaccess.thecvf.com/content_CVPRW_2020/html/w40/Bhalgat_LSQ_Improving_Low-Bit_Quantization_Through_Learnable_Offsets_and_Better_Initialization_CVPRW_2020_paper.html">paper</a>
                      accepted to the Efficient Deep Learning in Computer Vision workshop at CVPR 2020</li>
                    <li><span style="color:blue;">[02/2020]</span> <a
                        href="https://arxiv.org/abs/2003.00075">Preprint</a> for our work on "Learned Threshold Pruning"
                      available.</li>
                    <li><span style="color:blue;">[11/2019]</span> 3rd position in NeurIPS 2019 MicroNet competition
                      ImageNet track! - <a href="https://micronet-challenge.github.io/leaderboard.html">Leaderboard</a>.
                      Code: <a href="https://github.com/yashbhalgat/QualcommAI-MicroNet-submission-MixNet">here</a> and
                      <a href="https://github.com/yashbhalgat/QualcommAI-MicroNet-submission-EfficientNet">here</a>.
                    </li>
                  </ul>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Selected Publications</heading>
                  <span> (Full list: <a href="https://scholar.google.com/citations?hl=en&user=q0VSEHYAAAAJ"><i><u>Google Scholar</i></u></a>)</span>
                </td>
              </tr>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"
            id="research">
            <tbody>

              <tr onmouseout="egoseg_stop()" onmouseover="egoseg_start()">
                <td style="padding:20px;width:33%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='egoseg_image'>
                      <img src='images/kettle_knife_comp.gif' width=240>
                    </div>
                    <img src='images/method.jpg' width=240>
                  </div>
                  <script type="text/javascript">
                    function egoseg_start() {
                      document.getElementById('egoseg_image').style.opacity = "1";
                    }

                    function egoseg_stop() {
                      document.getElementById('egoseg_image').style.opacity = "0";
                    }
                    egoseg_stop()
                  </script>
                </td>
                <td style="padding:20px;width:67%;vertical-align:middle">
                  <a href="https://www.robots.ox.ac.uk/~vadim/egoseg3d/">
                    <papertitle>3D-Aware Instance Segmentation and Tracking in Egocentric Videos
                    </papertitle>
                  </a>
                  <br>
                  <strong>Yash Bhalgat*</strong>,
                  <a href="https://github.com/dichotomies" target="_blank" rel="noopener noreferrer">Vadim Tschernezki*</a>,
                  <a href="https://eng.ox.ac.uk/people/iro-laina/" target="_blank" rel="noopener noreferrer">Iro
                    Laina</a>,
                  <a href="https://www.robots.ox.ac.uk/~joao/" target="_blank" rel="noopener noreferrer">Jo&atilde;o
                    Henriques</a>,
                  <a href="https://www.robots.ox.ac.uk/~vedaldi/" target="_blank" rel="noopener noreferrer">Andrea
                      Vedaldi</a>
                  <a href="https://www.robots.ox.ac.uk/~az/" target="_blank" rel="noopener noreferrer">Andrew
                    Zisserman</a>,
                  <br>
                  <em>ACCV</em>, 2024
                  <br>
                  <p>We propose a 3D-aware method for object tracking in <i>long</i> egocentric videos, leveraging scene geometry
                    to handle rapid motion and occlusions. Our approach improves tracking accuracy, reduces ID switches by up to 80%,
                    and enables applications like 3D object reconstruction and amodal segmentation.</p>
                </td>
              </tr>
              
              <tr onmouseout="n2f2_stop()" onmouseover="n2f2_start()">
                <td style="padding:20px;width:33%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='n2f2_image'>
                      <img src='images/toy_chair.gif' width=240>
                    </div>
                    <img src='images/original.gif' width=240>
                  </div>
                  <script type="text/javascript">
                    function n2f2_start() {
                      document.getElementById('n2f2_image').style.opacity = "1";
                    }

                    function n2f2_stop() {
                      document.getElementById('n2f2_image').style.opacity = "0";
                    }
                    n2f2_stop()
                  </script>
                </td>
                <td style="padding:20px;width:67%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2403.10997">
                    <papertitle>N2F2: Hierarchical Scene Understanding with Nested Neural Feature Fields
                    </papertitle>
                  </a>
                  <br>
                  <strong>Yash Bhalgat</strong>,
                  <a href="https://eng.ox.ac.uk/people/iro-laina/" target="_blank" rel="noopener noreferrer">Iro
                    Laina</a>,
                  <a href="https://www.robots.ox.ac.uk/~joao/" target="_blank" rel="noopener noreferrer">Jo&atilde;o
                    Henriques</a>,
                  <a href="https://www.robots.ox.ac.uk/~az/" target="_blank" rel="noopener noreferrer">Andrew
                    Zisserman</a>,
                  <a href="https://www.robots.ox.ac.uk/~vedaldi/" target="_blank" rel="noopener noreferrer">Andrea
                    Vedaldi</a>
                  <br>
                  <em>ECCV</em>, 2024
                  <br>
                  <p>We present Nested Neural Feature Fields (N2F2), a hierarchical approach to 3D scene understanding that encodes 
                    multi-scale properties in a unified feature field. Our method outperforms state-of-the-art approaches like LERF 
                    and LangSplat on open-vocabulary 3D tasks, especially for complex queries, while enabling faster inference.</p>
                </td>
              </tr>

              <tr onmouseout="cont_stop()" onmouseover="cont_start()">
                <td style="padding:20px;width:33%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='cont_image'>
                      <img src='images/cont_viz.gif' width=240>
                    </div>
                    <img src='images/cont_viz_small.png' width=240>
                  </div>
                  <script type="text/javascript">
                    function cont_start() {
                      document.getElementById('cont_image').style.opacity = "1";
                    }

                    function cont_stop() {
                      document.getElementById('cont_image').style.opacity = "0";
                    }
                    cont_stop()
                  </script>
                </td>
                <td style="padding:20px;width:67%;vertical-align:middle">
                  <a href="https://www.robots.ox.ac.uk/~vgg/research/contrastive-lift/">
                    <papertitle>Contrastive Lift: 3D Object Instance Segmentation by Slow-Fast Contrastive Fusion
                    </papertitle>
                  </a> <a href="https://github.com/yashbhalgat/Contrastive-Lift">[Code]</a>
                  <br>
                  <strong>Yash Bhalgat</strong>,
                  <a href="https://eng.ox.ac.uk/people/iro-laina/" target="_blank" rel="noopener noreferrer">Iro
                    Laina</a>,
                  <a href="https://www.robots.ox.ac.uk/~joao/" target="_blank" rel="noopener noreferrer">Jo&atilde;o
                    Henriques</a>,
                  <a href="https://www.robots.ox.ac.uk/~az/" target="_blank" rel="noopener noreferrer">Andrew
                    Zisserman</a>,
                  <a href="https://www.robots.ox.ac.uk/~vedaldi/" target="_blank" rel="noopener noreferrer">Andrea
                    Vedaldi</a>
                  <br>
                  <em>NeurIPS</em>, 2023 &nbsp; <b><span style="color:crimson;">(Spotlight presentation)</span></b>
                  <br>
                  <p>We present a novel "slow-fast" contrastive fusion method to lift 2D predictions to 3D for scalable
                    instance segmentation, achieving significant improvements without requiring an upper bound on the
                    number of objects in the scene.</p>
                </td>
              </tr>

              <tr onmouseout="epi_stop()" onmouseover="epi_start()">
                <td style="padding:20px;width:33%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='epi_image'>
                      <img src='images/epi_main.PNG' width=240>
                    </div>
                    <img src='images/epi_viz.PNG' width=240>
                  </div>
                  <script type="text/javascript">
                    function epi_start() {
                      document.getElementById('epi_image').style.opacity = "1";
                    }

                    function epi_stop() {
                      document.getElementById('epi_image').style.opacity = "0";
                    }
                    epi_stop()
                  </script>
                </td>
                <td style="padding:20px;width:67%;vertical-align:middle">
                  <a href="https://www.robots.ox.ac.uk/~vgg/research/light-touch/">
                    <papertitle>A Light Touch Approach to Teaching Transformers Multi-view Geometry
                    </papertitle>
                  </a>
                  <br>
                  <strong>Yash Bhalgat</strong>,
                  <a href="https://www.robots.ox.ac.uk/~joao/" target="_blank" rel="noopener noreferrer">Jo&atilde;o
                    Henriques</a>,
                  <a href="https://www.robots.ox.ac.uk/~az/" target="_blank" rel="noopener noreferrer">Andrew
                    Zisserman</a>
                  <br>
                  <em>CVPR</em>, 2023
                  <br>
                  <p>An "Epipolar-guided training" method to incorporate multi-view geometric priors into Transformer
                    models, which can be implemented in <em>150 lines of code</em>.
                    <!--We show that: (1) by training a multi-view transformer (such as <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Tan_Instance-Level_Image_Retrieval_Using_Reranking_Transformers_ICCV_2021_paper.pdf" target="_blank" rel="noopener noreferrer">RRT</a>) with this loss, the transformer model learns to use geometric priors during inference which leads to better performance, and (2) -->
                    During test-time, the Transformer implicitly estimates the epipolar geometry given 2 images and uses
                    it for downstream predictions, e.g. for pose-invariant retrieval.</p>
                </td>
              </tr>

              <tr onmouseout="dirnet_stop()" onmouseover="dirnet_start()">
                <td style="padding:20px;width:33%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='dirnet_image'>
                      <img src='images/dirnet_other_padded.PNG' width=240>
                    </div>
                    <img src='images/dirnet.PNG' width=240>
                  </div>
                  <script type="text/javascript">
                    function dirnet_start() {
                      document.getElementById('dirnet_image').style.opacity = "1";
                    }

                    function dirnet_stop() {
                      document.getElementById('dirnet_image').style.opacity = "0";
                    }
                    dirnet_stop()
                  </script>
                </td>
                <td style="padding:20px;width:67%;vertical-align:middle">
                  <a
                    href="https://openaccess.thecvf.com/content/WACV2022/papers/Yang_Dynamic_Iterative_Refinement_for_Efficient_3D_Hand_Pose_Estimation_WACV_2022_paper.pdf">
                    <papertitle>Dynamic Iterative Refinement for Efficient 3D Hand Pose Estimation</papertitle>
                  </a>
                  <br>
                  <a href="https://scholar.google.com/citations?user=A4_2BW4AAAAJ&hl=en">John Yang</a>,
                  <strong>Yash Bhalgat</strong>,
                  <a href="https://scholar.google.com/citations?hl=en&user=0-tF1dwAAAAJ">Simyung Chang</a>,
                  <a href="http://www.porikli.com/">Fatih Porikli</a>,
                  <a href="https://scholar.google.com/citations?user=h_8-1M0AAAAJ&hl=en">Nojun Kwak</a>
                  <br>
                  <em>WACV</em>, 2022
                  <br>
                  <p>We propose a tiny deep network of which partial layers are recursively exploited for refining its
                    previous estimations. During its iterative refinements, we employ learned gating criteria to decide
                    whether to exit from the weight-sharing loop, allowing per-sample adaptation in our model. We also
                    predict and exploit uncertainty estimations in the gating mechanism.</p>
                </td>
              </tr>

              <tr onmouseout="struct_stop()" onmouseover="struct_start()">
                <td style="padding:20px;width:33%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='struct_image'>
                      <img src='images/struct_decomp_padded.PNG' width=240>
                    </div>
                    <img src='images/struct_other.PNG' width=240>
                  </div>
                  <script type="text/javascript">
                    function struct_start() {
                      document.getElementById('struct_image').style.opacity = "1";
                    }

                    function struct_stop() {
                      document.getElementById('struct_image').style.opacity = "0";
                    }
                    struct_stop()
                  </script>
                </td>
                <td style="padding:20px;width:67%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/2008.02454.pdf">
                    <papertitle>Structured Convolutions for Efficient Neural Network Design</papertitle>
                  </a>
                  <br>
                  <strong>Yash Bhalgat</strong>,
                  <a href="https://scholar.google.com/citations?user=u93s1AUAAAAJ&hl=en">Yizhe Zhang</a>,
                  <a href="https://www.linkedin.com/in/jmjlin">Jamie Lin</a>,
                  <a href="http://www.porikli.com/">Fatih Porikli</a>
                  <br>
                  <em>NeurIPS</em>, 2020
                  <br>
                  <p>We introduce a neat trick to enable the execution of convolution operations in the form of
                    efficient, scaled, sum-pooling components. We present a Structural Regularization loss that enables
                    this decomposition with negligible performance loss. Our method is competitive with other tensor
                    decomposition and structured pruning methods.</p>
                </td>
              </tr>

              <tr onmouseout="datainit_stop()" onmouseover="datainit_start()">
                <td style="padding:20px;width:33%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='datainit_image'>
                      <img src='images/datainit.png' width=240>
                    </div>
                    <img src='images/datainit_other.png' width=240>
                  </div>
                  <script type="text/javascript">
                    function datainit_start() {
                      document.getElementById('datainit_image').style.opacity = "1";
                    }

                    function datainit_stop() {
                      document.getElementById('datainit_image').style.opacity = "0";
                    }
                    datainit_stop()
                  </script>
                </td>
                <td style="padding:20px;width:67%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/2105.10335.pdf">
                    <papertitle>Data-driven Weight Initialization with Sylvester Solvers
                    </papertitle>
                  </a>
                  <br>
                  <a href="https://scholar.google.com/citations?user=0tP8MuMAAAAJ&hl=en">Debasmit Das</a>,
                  <strong>Yash Bhalgat</strong>,
                  <a href="http://www.porikli.com/">Fatih Porikli</a>
                  <br>
                  <em>Practical Machine Learning for Developing Countries Workshop, ICLR</em>, 2021
                  <br>
                  <p>We propose a data-driven scheme to initialize the parameters of a neural network. The
                    initialization is cast as an optimization problem, which is restructured into the well-known
                    Sylvester equation that has fast and efficient gradient-free solutions. We show that our proposed
                    method is especially effective in few-shot and fine-tuning settings.</p>
                </td>
              </tr>

              <tr onmouseout="ff_stop()" onmouseover="ff_start()">
                <td style="padding:20px;width:33%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='ff_image'>
                      <img src='images/lsq_plus.png' width=240>
                    </div>
                    <img src='images/lsq_plus_other.png' width=240>
                  </div>
                  <script type="text/javascript">
                    function ff_start() {
                      document.getElementById('ff_image').style.opacity = "1";
                    }

                    function ff_stop() {
                      document.getElementById('ff_image').style.opacity = "0";
                    }
                    ff_stop()
                  </script>
                </td>
                <td style="padding:20px;width:67%;vertical-align:middle">
                  <a
                    href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w40/Bhalgat_LSQ_Improving_Low-Bit_Quantization_Through_Learnable_Offsets_and_Better_Initialization_CVPRW_2020_paper.pdf">
                    <papertitle>LSQ+: Improving low-bit quantization through learnable offsets and better initialization
                    </papertitle>
                  </a>
                  <br>
                  <strong>Yash Bhalgat</strong>,
                  <a href="https://www.linkedin.com/in/jinwon-lee-6ba13a7">Jinwon Lee</a>,
                  <a href="https://scholar.google.com/citations?user=akNuBBEAAAAJ&hl=en">Markus Nagel</a>,
                  <a href="https://scholar.google.com/citations?user=OGEyrG8AAAAJ&hl=en">Tijmen Blankevoort</a>,
                  <a href="https://scholar.google.com/citations?user=h_8-1M0AAAAJ&hl=en">Nojun Kwak</a>
                  <br>
                  <em>Efficient Deep Learning in Computer Vision Workshop, CVPR</em>, 2020
                  <br>
                  <p>We introduce a general asymmetric quantization scheme with trainable scale and offset parameters.
                    LSQ+ shows SOTA results for EfficientNet and MixNet outperforming LSQ for low-bit quantization.</p>
                </td>
              </tr>

              <tr onmouseout="ltp_stop()" onmouseover="ltp_start()">
                <td style="padding:20px;width:33%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='ltp_image'>
                      <img src='images/ltp.png' width=240>
                    </div>
                    <img src='images/ltp_other.png' width=240>
                  </div>
                  <script type="text/javascript">
                    function ltp_start() {
                      document.getElementById('ltp_image').style.opacity = "1";
                    }

                    function ltp_stop() {
                      document.getElementById('ltp_image').style.opacity = "0";
                    }
                    ltp_stop()
                  </script>
                </td>
                <td style="padding:20px;width:67%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/2003.00075">
                    <papertitle>Learned Threshold Pruning</papertitle>
                  </a>
                  <br>
                  <a href="https://dblp.uni-trier.de/pers/a/Azarian:Kambiz.html">Kambiz Azarian</a>,
                  <strong>Yash Bhalgat</strong>,
                  <a href="https://www.linkedin.com/in/jinwon-lee-6ba13a7">Jinwon Lee</a>,
                  <a href="https://scholar.google.com/citations?user=OGEyrG8AAAAJ&hl=en">Tijmen Blankevoort</a>
                  <br>
                  <em><a href="https://arxiv.org/pdf/2003.00075">arxiv</a></em>, 2020
                  <br>
                  <p>We propose an end-to-end differentiable method for learning layerwise pruning thresholds which
                    results in SOTA model compression ratios with AlexNet, ResNet and EfficientNet. Our method also
                    generates a trail of checkpoints with different accuracy-efficiency operating points.</p>
                </td>
              </tr>

              <tr onmouseout="qkd_stop()" onmouseover="qkd_start()">
                <td style="padding:20px;width:33%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='qkd_image'>
                      <img src='images/qkd.png' width=240>
                    </div>
                    <img src='images/qkd_other.png' width=240>
                  </div>
                  <script type="text/javascript">
                    function qkd_start() {
                      document.getElementById('qkd_image').style.opacity = "1";
                    }

                    function qkd_stop() {
                      document.getElementById('qkd_image').style.opacity = "0";
                    }
                    qkd_stop()
                  </script>
                </td>
                <td style="padding:20px;width:67%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/1911.12491">
                    <papertitle>QKD: Quantization-aware Knowledge Distillation for Low-bit Quantization</papertitle>
                  </a>
                  <br>
                  <strong>Yash Bhalgat*</strong>,
                  <a href="https://scholar.google.co.kr/citations?user=mzFEJVIAAAAJ&hl=ko">Jangho Kim*</a>,
                  <a href="https://www.linkedin.com/in/jinwon-lee-6ba13a7">Jinwon Lee</a>,
                  <a href="https://www.linkedin.com/in/chirag-patel-b0a6921/">Chirag Patel</a>,
                  <a href="https://scholar.google.co.kr/citations?user=h_8-1M0AAAAJ&hl=ko">Nojun Kwak</a>
                  <br>
                  <em><a href="https://arxiv.org/abs/1911.12491">arxiv</a></em>, 2020
                  <br>
                  <p>Low-bit quantization and KD often don't go well together, but both are important approaches to
                    reduces a model's memory footprint. We propose an effective method to combine these two methods and
                    show results that outperform all existing quantization/KD approaches.</p>
                </td>
              </tr>

              <tr onmouseout="ibm_stop()" onmouseover="ibm_start()">
                <td style="padding:20px;width:33%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='ibm_image'>
                      <img src='images/ibm.png' width=240>
                    </div>
                    <img src='images/ibm.png' width=240>
                  </div>
                  <script type="text/javascript">
                    function ibm_start() {
                      document.getElementById('ibm_image').style.opacity = "1";
                    }

                    function ibm_stop() {
                      document.getElementById('ibm_image').style.opacity = "0";
                    }
                    ibm_stop()
                  </script>
                </td>
                <td style="padding:20px;width:67%;vertical-align:middle">
                  <a href="https://konvens.org/proceedings/2019/papers/KONVENS2019_paper_52.pdf">
                    <papertitle>Teacher-Student Learning Paradigm for Tri-training: An Efficient
                      Method for Unlabeled Data Exploitation</papertitle>
                  </a>
                  <br>
                  <strong>Yash Bhalgat</strong>,
                  <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-liuzh">Zhe Liu</a>,
                  <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-psgundec">Pritam
                    Gundecha</a>,
                  <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-jumahmud">Jalal Mahmud</a>,
                  <a href="https://www.linkedin.com/in/amita-misra/">Amita Misra</a>
                  <br>
                  <em>KONVENS</em>, 2019
                  <br>
                  <p>Teacher-student tri-training is a method for semi-supervised learning using 3 classifiers working
                    using adaptive teacher and student thresholds.</p>
                </td>
              </tr>

              <tr onmouseout="sms_stop()" onmouseover="sms_start()">
                <td style="padding:20px;width:33%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='sms_image'>
                      <img src='images/sms.png' width=200>
                    </div>
                    <img src='images/sms.png' width=200>
                  </div>
                  <script type="text/javascript">
                    function sms_start() {
                      document.getElementById('sms_image').style.opacity = "1";
                    }

                    function sms_stop() {
                      document.getElementById('sms_image').style.opacity = "0";
                    }
                    sms_stop()
                  </script>
                </td>
                <td style="padding:20px;width:67%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/1812.11302.pdf">
                    <papertitle>Annotation-cost Minimization for Medical Image Segmentation using Suggestive Mixed
                      Supervision Fully Convolutional Networks</papertitle>
                  </a>
                  <br>
                  <strong>Yash Bhalgat*</strong>,
                  <a href="https://meetshah1995.github.io/">Meet Shah*</a>
                  <a href="https://www.cse.iitb.ac.in/~suyash/">Suyash Awate</a>
                  <br>
                  <em>Medical Imaging meets NeurIPS workshop</em>, 2019
                  <br>
                  <p>For Medical Image segmentation, we present a budget-based cost-minimization framework in a
                    mixed-supervision setting via dense segmentations, bounding boxes, and landmarks.</p>
                </td>
              </tr>

              <tr onmouseout="icassp_stop()" onmouseover="icassp_start()">
                <td style="padding:20px;width:33%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='icassp_image'>
                      <img src='images/icassp.png' width=200>
                    </div>
                    <img src='images/icassp_other.png' width=200>
                  </div>
                  <script type="text/javascript">
                    function icassp_start() {
                      document.getElementById('icassp_image').style.opacity = "1";
                    }

                    function icassp_stop() {
                      document.getElementById('icassp_image').style.opacity = "0";
                    }
                    icassp_stop()
                  </script>
                </td>
                <td style="padding:20px;width:67%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/abstract/document/8462088">
                    <papertitle>CATSEYES: Categorizing Seismic structures with tessellated scattering wavelet networks
                    </papertitle>
                  </a>
                  <br>
                  <strong>Yash Bhalgat</strong>,
                  <a href="https://ieeexplore.ieee.org/author/37086455393">Jean Charlety</a>,
                  <a href="https://ieeexplore.ieee.org/author/37285084800">Laurent Duval</a>
                  <br>
                  <em>ICASSP</em>, 2018
                  <br>
                  <p>We use Scattering Wavelets transforms to extract sparse feature sets from seismic data. We show
                    that using this method combined with simple PCA-based feature selection leads to promising
                    classification performance in affordable computation time.</p>
                </td>
              </tr>

            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"
            id="invited-talks">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Invited Talks</heading>
                  <p>
                  <ul>
                    <li><span style="font-size: 1.1em; font-weight: bold;">"From <b><i>Video</i></b> to <b><i>Virtual</i></b>: Object-centric 3D scene understanding from videos"</span> 
                      <b>[<a href="https://docs.google.com/presentation/d/1eoZN7aM1BTUQxtRPZ0PDx_TtxzY6hE1sDYT_83MGWF8/edit?usp=sharing">Slides</a>]</b>
                      <ul style="list-style-type: none;">
                        <li>@ Google DeepMind India (hosted by <a href="https://www.prateekjain.org/">Prateek Jain</a>)</li>
                        <li>@ Adobe Research, Bangalore (hosted by <a href="https://research.adobe.com/person/balaji-vasan-srinivasan/">Balaji Vasan Srinivasan</a>)</li>
                        <li>@ Show Lab, National University of Singapore (hosted by <a href="https://sites.google.com/view/showlab">Mike Shou</a>)</li>
                        <li>@ CS Peer Talk, Peking University (hosted by <a href="https://cfcs.pku.edu.cn/english/events/cs_peer_talks/242079.htm">Turing Class Research Committee</a>)</li>
                        <li>@ CVIT, IIIT Hyderabad (hosted by <a href="https://makarandtapaswi.github.io/">Makarand Tapaswi</a> and <a href="https://faculty.iiit.ac.in/~vgandhi/">Vineet Gandhi</a>)</li>
                        <li>@ VAL, IISc Bangalore (hosted by <a href="https://cds.iisc.ac.in/faculty/venky/">R. Venkatesh Babu</a>)</li>
                        <li>@ TCS Research, Pune (hosted by <a href="https://scholar.google.co.in/citations?user=LeHCh80AAAAJ&hl=en">Shirish Karande</a>)</li>
                        <li>@ Avataar.ai (hosted by <a href="https://shubham-goel.github.io/">Shubham Goel</a> and <a href="https://www.linkedin.com/in/shubhamjain27/">Shubham Jain</a>)</li>
                      </ul>
                    </li>
                    <br>
                    <li><span style="font-size: 1.1em; font-weight: bold;">"Deep Learning for Computer Vision and applications to the Ghanaian context"</span> <b>[<a href="https://youtu.be/qkfwqiAygpk?si=dPbsH0mpDNhcJLNb">Youtube</a>]</b>
                      <ul style="list-style-type: none;">
                        <li>@ SPARK Deep Learning workshop at IndabaX Ghana</li>
                      </ul>
                    </li>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"
            id="hall-of-fame">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Hall of Fame</heading>
                  <p>
                  <ul>
                    <li>All India Rank <b>12</b> in IITJEE-Mains 2013 exam among 1.5 million students</li>
                    <li>All India Rank <b>155</b> in IITJEE-Advanced 2013 exam among 0.2 million students</li>
                    <li>Featured in National Top 30 for the International Astronomy Olympiad, 2013</li>
                    <li>All India Rank 60 and awarded the KVPY Scholarship by Govt. of India</li>
                    <li>Among top 300 in India to compete in the Physics, Chemistry and Mathematics olympiads</li>
                    <li>Cargill Global Scholarship 2014-15 and selected in the 10-member Indian cohort to represent at
                      the global seminar in Minneapolis, USA in 2016</li>
                    <li>Undergraduate Research Award (URA02) for Bachelors thesis at IIT Bombay</li>
                  </ul>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"
            id="music">
            <tbody>
              <tr>
                <td style="padding:2.5%;width:50%;vertical-align:middle">
                  <heading>Music</heading>
                  <p>I play the <a href="https://www.youtube.com/watch?v=rPPkRgw532I">Tabla</a>, an Indian percussion
                    instrument and have a <i>Visharad</i> (&asymp; Bachelor of Music) in Indian Classical music. I've
                    also briefly tried to learn the Piano and Harmonica. I am a natural beatboxer (<a
                      href="https://youtu.be/0ZfSStpZPfI">check this out</a>) and I sometimes post music videos here:
                  </p>
                  <ul>
                    <li><a href="https://www.instagram.com/yashbhalgat.music/">Instagram Music Channel</a></li>
                    <li><a href="https://m.youtube.com/c/YashBhalgat">YouTube Music Channel</a></li>
                  </ul>
                </td>
                <td style="padding:2.5%;width:30%;max-width:30%" align="right">
                  <a href="images/big_tabla.jpg"><img style="width:80%;max-width:100%" alt="Yash playing Tabla"
                      src="images/big_tabla.jpg" class="hoverZoomLink"></a>
                </td>
                <!-- <td style="padding:0px;width:50%;vertical-align:right">
                  <img src='images/big_tabla.jpg' width=300></div> -->
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"
            cellspacing="0" cellpadding="20" id="office-hours">
            <tbody>
              <tr>
                <td>
                  <heading>Office Hours</heading>
                  <!-- Calendly inline widget begin -->
                  <!-- <div class="calendly-inline-widget" data-url="https://calendly.com/yashbhalgat/public-office-hours"
                    style="min-width:320px;height:630px;"></div>
                  <script type="text/javascript" src="https://assets.calendly.com/assets/external/widget.js"
                    async></script> -->
                  <!-- Calendly inline widget end -->
                  <!-- Cal inline embed code begins -->
                  <div style="width:100%;height:100%;overflow:scroll" id="my-cal-inline"></div>
                  <script type="text/javascript">
                    (function (C, A, L) { let p = function (a, ar) { a.q.push(ar); }; let d = C.document; C.Cal = C.Cal || function () { let cal = C.Cal; let ar = arguments; if (!cal.loaded) { cal.ns = {}; cal.q = cal.q || []; d.head.appendChild(d.createElement("script")).src = A; cal.loaded = true; } if (ar[0] === L) { const api = function () { p(api, arguments); }; const namespace = ar[1]; api.q = api.q || []; if(typeof namespace === "string"){cal.ns[namespace] = cal.ns[namespace] || api;p(cal.ns[namespace], ar);p(cal, ["initNamespace", namespace]);} else p(cal, ar); return;} p(cal, ar); }; })(window, "https://app.cal.com/embed/embed.js", "init");
                  Cal("init", "30min", {origin:"https://cal.com"});

                    Cal.ns["30min"]("inline", {
                      elementOrSelector:"#my-cal-inline",
                      config: {"layout":"month_view"},
                      calLink: "yash-bhalgat/30min",
                    });

                    Cal.ns["30min"]("ui", {"styles":{"branding":{"brandColor":"#000000"}},"hideEventTypeDetails":false,"layout":"month_view"});
                    </script>
                    <!-- Cal inline embed code ends -->
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    Website template borrowed from <a href="https://jonbarron.info">here</a>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>

  <!--script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=ppNj2MmKq82N8uj2zzgmrUkPLcDDci58wlNn-LWLsBo"></script-->
  <script>
    document.addEventListener('DOMContentLoaded', function() {
        const navToggle = document.querySelector('.nav-toggle');
        const navMenu = document.querySelector('.navbar ul');

        navToggle.addEventListener('click', function() {
            navMenu.classList.toggle('show');
            navToggle.classList.toggle('active');
        });

        // Close menu when clicking a link
        document.querySelectorAll('.navbar ul li a').forEach(link => {
            link.addEventListener('click', () => {
                navMenu.classList.remove('show');
                navToggle.classList.remove('active');
            });
        });
    });
  </script>

</body>

</html>
